{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebb308a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "from datasets import load_dataset\n",
    "from transformers import HfArgumentParser\n",
    "\n",
    "import pandas as pd\n",
    "import os,pickle,time, random\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "\n",
    "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff1045ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_coin():\n",
    "    coin = random.randint(0,1)\n",
    "    #https://github.com/globien/easy-python/blob/master/102_%E6%8A%9B%E7%A1%AC%E5%B8%81%E8%BF%9E%E7%BB%AD%E6%AD%A3%E9%9D%A2%E9%97%AE%E9%A2%98.py\n",
    "    if coin== 1:                       # 扔到了一次正面\n",
    "        return True\n",
    "    else:\n",
    "    #     print('tail')                    # 不是正面，重置计数器\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b5909b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pkl_data(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    return data\n",
    "\n",
    "def preprocessing(data):\n",
    "    \n",
    "    new = [] # datapoints has 2 or more golden translation\n",
    "    gold = []\n",
    "    for item in tqdm(data):\n",
    "        tmp={}\n",
    "        text= item.strip('\\n')\n",
    "        tmp['English'] = text\n",
    "\n",
    "        tmp_hinglish=[]\n",
    "        for item in data[item]:\n",
    "            tmp_hinglish.append(item.strip('\\n'))\n",
    "\n",
    "        tmp['Gold_Hinglish'] = tmp_hinglish\n",
    "\n",
    "        new.append(tmp)\n",
    "        gold.append(tmp_hinglish)\n",
    "    return new,gold\n",
    "\n",
    "def transform_unified_golden_label(data_valid):\n",
    "    gold_label=[]\n",
    "\n",
    "    for item in data_valid:\n",
    "        tmp=[]\n",
    "        for i in data_valid[item]:\n",
    "            tmp.append(i.strip('\\n'))\n",
    "        while len(tmp)<7:\n",
    "            tmp.append('')\n",
    "        gold_label.append(tmp)\n",
    "        \n",
    "    return gold_label\n",
    "\n",
    "def post_processing(data):\n",
    "    res = []\n",
    "    for sent in data:\n",
    "\n",
    "        if \"[Hinglish]:\" in sent:\n",
    "            sent = sent.split(\"[Hinglish]:\")[-1]\n",
    "        if \"Hindi-English]:\" in sent:\n",
    "            sent = sent.split(\"Hindi-English]:\")[-1]\n",
    "        if \"Hindi-English:\" in sent:\n",
    "            sent = sent.split(\"Hindi-English:\")[-1]\n",
    "        if \"Hinglish translation:\" in sent:\n",
    "            sent = sent.split(\"Hinglish translation:\")[-1]\n",
    "\n",
    "        res.append(sent.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"(\",\"\").replace(\")\",\"\").replace(\"\\n\",\"\").strip())\n",
    "\n",
    "    return res\n",
    "\n",
    "def post_processing_v2(data):\n",
    "    '''\n",
    "    created by Aditya\n",
    "    '''\n",
    "    res = []\n",
    "    for sent in data:\n",
    "        if \"[Hinglish]:\" in sent:\n",
    "            sent = sent.split(\"[Hinglish]:\")[-1]\n",
    "        if \"Hindi-English]:\" in sent:\n",
    "            sent = sent.split(\"Hindi-English]:\")[-1]\n",
    "        if \"Hindi-English:\" in sent:\n",
    "            sent = sent.split(\"Hindi-English:\")[-1]\n",
    "        if \"Hinglish translation:\" in sent:\n",
    "            sent = sent.split(\"Hinglish translation:\")[-1]\n",
    "        if \"[English] se [Hinglish] mein:\" in sent:\n",
    "            sent = sent.split(\"[English] se [Hinglish] mein:\")[-1]\n",
    "        if \"Hinglish:\" in sent:\n",
    "            sent = sent.split(\"Hinglish:\")[-1]\n",
    "        if \"[English]:\" in sent:\n",
    "            sent = sent.split(\"[English]:\")[-1]\n",
    "        if \"Hinglish Translation:\" in sent: #similar to line 13 we can change this to lowercase \n",
    "            sent = sent.split(\"Hinglish Translation:\")[-1]\n",
    "        if \"hum is prakaar keh sakte hain:\" in sent: #from line 690\n",
    "            sent = sent.split(\"hum is prakaar keh sakte hain:\")[-1]\n",
    "        if \"[Translation in Hinglish]:\" in sent: #from line 746\n",
    "            sent = sent.split(\"[Translation in Hinglish]:\")[-1]\n",
    "        if \"Note:\" in sent: #keep everything before \"(Note:\" #line 494\n",
    "            sent = sent.split(\"Note:\")[0]\n",
    "        if \"(Translation:\" in sent: #line 998\n",
    "            sent = sent.split(\"(Translation:\")[0]\n",
    "        if \"Hinglish mein hai:\" in sent: #from line 1054\n",
    "            sent = sent.split(\"Hinglish mein hai:\")[-1]\n",
    "        if \"Explanation:\" in sent: #line 1131\n",
    "            sent = sent.split(\"Explanation:\")[0]\n",
    "        if \"Translation Breakdown:\" in sent: #line 1516\n",
    "            sent = sent.split(\"Translation Breakdown:\")[0]\n",
    "        if \"[English] to [Hinglish] translation:\" in sent: #line 1705\n",
    "            sent = sent.split(\"[English] to [Hinglish] translation:\")[-1]\n",
    "        if \"[English to Hinglish translation]\" in sent: #line 1789\n",
    "            sent = sent.split(\"[English to Hinglish translation]\")[-1]\n",
    "        if \"[English to Hinglish]:\" in sent: #line 2111\n",
    "            sent = sent.split(\"[English to Hinglish translation]\")[-1]\n",
    "        if \"(Hinglish translation)\" in sent: #line 2461\n",
    "            sent = sent.split(\"(Hinglish translation)\")[-1]\n",
    "        if \"(In Hinglish\" in sent: #line 1131\n",
    "            sent = sent.split(\"(In Hinglish\")[0]\n",
    "        if \"[Hinglish] mein yeh anuvad hai:\" in sent: #line 648\n",
    "            sent = sent.split(\"[Hinglish] mein yeh anuvad hai:\")[-1]\n",
    "        res.append(sent.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"(\",\"\").replace(\")\",\"\").replace(\"\\n\",\"\").strip())\n",
    "    return res\n",
    "\n",
    "\n",
    "def request_Gemini(dataset,prompt,value_temp=0.1,model_id=\"gemini-1.5-flash\"):\n",
    "    res=[]\n",
    "    res_flip=[]\n",
    "    # ref:https://ai.google.dev/gemini-api/docs/quickstart?lang=python\n",
    "    genai.configure(api_key=\"\")\n",
    "\n",
    "    #safety check:https://ai.google.dev/gemini-api/docs/safety-settings\n",
    "    safety_settings = {HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "                        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "                        }\n",
    "#     system_role=\"You are a translation expert, and I need your help in impartially judging the quality of two translations. \"\n",
    "    model = genai.GenerativeModel(model_name=model_id,\n",
    "#                                   system_instruction=system_role,\n",
    "                                  generation_config = genai.GenerationConfig(temperature=value_temp,),\n",
    "                                  safety_settings=safety_settings,\n",
    "                                 )\n",
    "    \n",
    "    for item in tqdm(dataset):\n",
    "       \n",
    "        time.sleep(4) # wait for 4 seconds,and then move to next step\n",
    "        # form the prompt\n",
    "        result_flip_coin = flip_coin()\n",
    "        if result_flip_coin:\n",
    "            final_prompt = prompt.format(original_sent=item['English'],\n",
    "                                         first_sent=item['translation_reference'],\n",
    "                                         second_sent=item['translation_model'])\n",
    "        else:\n",
    "            final_prompt = prompt.format(original_sent=item['English'],\n",
    "                                         first_sent=item['translation_model'],\n",
    "                                         second_sent=item['translation_reference'])\n",
    "    \n",
    "        response = model.generate_content(final_prompt)\n",
    "        final_response = response.text\n",
    "        res.append(final_response)\n",
    "        res_flip.append(result_flip_coin)\n",
    "    \n",
    "    return res,res_flip\n",
    "\n",
    "def transform_digital_result_old(data):\n",
    "    res = []\n",
    "    for item in tqdm(data):\n",
    "        if \"Translation_2 \\n\" in item:\n",
    "            res.append(1)\n",
    "        elif \"Translation_1 \\n\" in item:\n",
    "            res.append(0)\n",
    "\n",
    "    return res\n",
    "\n",
    "def transform_digital_result(data,res_flip):\n",
    "    '''\n",
    "    if original_translation is better, annotate as 0\n",
    "    if PPO_translation is better, annotate as 1\n",
    "    '''\n",
    "    res = []\n",
    "    for item in tqdm(zip(data,res_flip)):\n",
    "        if item[1]:\n",
    "            if \"Translation_2 \\n\" in item[0]:\n",
    "                res.append(1)\n",
    "            elif \"Translation_1 \\n\" in item[0]:\n",
    "                res.append(0)\n",
    "        else:\n",
    "            if \"Translation_2 \\n\" in item[0]:\n",
    "                res.append(0)\n",
    "            elif \"Translation_1 \\n\" in item[0]:\n",
    "                res.append(1)\n",
    "            \n",
    "\n",
    "    return res\n",
    "\n",
    "def calculate_new_model_win_rate(result):\n",
    "    result=result.to_list()\n",
    "    win_rate=sum(result)/len(result)\n",
    "    \n",
    "    print(\"The win rate of our model is: {:.4f}\".format(win_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "38921f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_chrf(prediction,reference):\n",
    "    '''\n",
    "    evaluate translation quality using  chrF\n",
    "    '''\n",
    "    \n",
    "    chrf = evaluate.load(\"chrf\") # chrF\n",
    "    results = chrf.compute(predictions=prediction, references=reference)\n",
    "    return results['score']\n",
    "\n",
    "def evaluate_chrf_plus_plus(prediction,reference):\n",
    "    '''\n",
    "    evaluate translation quality using  chrF++\n",
    "    \n",
    "    chrF+ (where word_order=1) and chrF++ (where word_order=2) produce scores that correlate better \n",
    "    with human judgements than chrF (where word_order=0) does.\n",
    "    https://huggingface.co/spaces/evaluate-metric/chrf\n",
    "    '''\n",
    "    chrf = evaluate.load(\"chrf\") # chrF++\n",
    "    results = chrf.compute(predictions=prediction,references=reference,word_order=2)\n",
    "    return results['score']\n",
    "\n",
    "def evaluate_comet(lst_prediction,lst_reference,lst_source):\n",
    "    '''\n",
    "    evaluate translation quality using comet\n",
    "    '''\n",
    "    \n",
    "    comet_metric = evaluate.load('comet') \n",
    "    results = comet_metric.compute(predictions=lst_prediction, references=lst_reference, sources=lst_source)\n",
    "\n",
    "    # print('The COMET score of test set is {:.2f}'.format(results['mean_score']))\n",
    "    return results['mean_score']\n",
    "\n",
    "def calculate_cosine_similarity(reference_embedding,generated_embedding):\n",
    "    \n",
    "    similarity_score = np.dot(generated_embedding, reference_embedding) / (np.linalg.norm(generated_embedding) * np.linalg.norm(reference_embedding))\n",
    "    # Ensure non-negative score\n",
    "    return max(similarity_score, 0)\n",
    "\n",
    "def evaluate_bleu(generated_text: str, reference_text: str, is_japanese: bool = False) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the BLEU score for a generated text compared to a reference truth text. This function supports\n",
    "    both general text and Japanese-specific evaluation by using the sacrebleu library.\n",
    "\n",
    "    Parameters:\n",
    "    - generated_text (str): The generated text to be evaluated.\n",
    "    - reference_text (str): The reference truth text.\n",
    "    - is_japanese (bool, optional): Flag to indicate whether the text is in Japanese, requiring special tokenization.\n",
    "\n",
    "    Returns:\n",
    "    - float: The BLEU score as a percentage (0 to 1 scale) for the generated text against the reference truth.\n",
    "    \"\"\"\n",
    "    sacrebleu = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "    # Compute BLEU score with or without Japanese-specific tokenization\n",
    "    bleu_args = {\"predictions\": generated_text, \"references\": reference_text, \"lowercase\": False}\n",
    "    \n",
    "    if is_japanese:\n",
    "        bleu_args[\"tokenize\"] = \"ja-mecab\"\n",
    "        bleu_args[\"lowercase\"] = True\n",
    "        \n",
    "    score = sacrebleu.compute(**bleu_args)[\"score\"]\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1708daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the test set\n",
    "path_golden = '../../data/MixMT-2022/valid_human_generated.pkl'\n",
    "data_golden = load_pkl_data(path_golden)\n",
    "gold_label=transform_unified_golden_label(data_golden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2b424cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the reference completions\n",
    "path_reference = './Meta-Llama-3.1-8B-Instruct-temp0.1.csv'\n",
    "dataset_reference = pd.read_csv(path_reference)  \n",
    "column_name_reference = os.path.split(path_reference)[1].split(\".csv\")[0]\n",
    "reference_completions = dataset_reference[column_name_reference].to_list()\n",
    "# res_reference = post_processing(reference_completions)\n",
    "res_reference = post_processing_v2(reference_completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c98574bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the model completions\n",
    "path_model = './llama3.1_merge-temp0.1.csv'\n",
    "dataset_model = pd.read_csv(path_model)  \n",
    "column_name_model = os.path.split(path_model)[1].split(\".csv\")[0]\n",
    "model_completions = dataset_model[column_name_model].to_list()\n",
    "# res_model = post_processing(model_completions)\n",
    "res_model = post_processing_v2(model_completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97210a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"######\")\n",
    "print(\"The BLEU score of original-model translation is: {:.2f}\".format(evaluate_bleu(res_reference, gold_label, is_japanese=False)))\n",
    "print(\"The BLEU score of ppo-model translation is: {:.2f}\".format(evaluate_bleu(res_model, gold_label, is_japanese=False)))\n",
    "\n",
    "print(\"######\")\n",
    "print(\"The CRF score of original-model translation is: {:.2f}\".format(evaluate_chrf(res_reference, gold_label)))\n",
    "print(\"The CRF score of ppo-model translation is: {:.2f}\".format(evaluate_chrf(res_model, gold_label)))\n",
    "\n",
    "\n",
    "print(\"######\")\n",
    "print(\"The CRF++ score of original-model translation is: {:.2f}\".format(evaluate_chrf_plus_plus(res_reference, gold_label)))\n",
    "print(\"The CRF++ score of ppo-model translation is: {:.2f}\".format(evaluate_chrf_plus_plus(res_model, gold_label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460b0663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "lst_reference=[]\n",
    "for item in gold_label:\n",
    "    lst_reference.append(item[0])\n",
    "\n",
    "lst_source = dataset_model['English'].to_list()\n",
    "\n",
    "print(\"The COMET score of original-model translation is: {:.2f}\".format(evaluate_comet(res_reference,lst_reference,lst_source)))\n",
    "print(\"The COMET score of ppo-model translation is: {:.2f}\".format(evaluate_comet(res_model,lst_reference,lst_source)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09a0988",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
